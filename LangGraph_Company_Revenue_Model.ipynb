{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies and Libraries**"
      ],
      "metadata": {
        "id": "3_iICbCKPJ2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jxTJKCmZPG8s"
      },
      "outputs": [],
      "source": [
        "!pip install -U -qq langchain langgraph langchain_openai langchain_community faiss-cpu\n",
        "!pip install -U -qq pypdf\n",
        "\n",
        "import os\n",
        "from typing import TypedDict, List\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup API Key**"
      ],
      "metadata": {
        "id": "A173Duo9SOWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: Read from environment variable (preferred)\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Option 2: Ask user to input if not set\n",
        "if not api_key:\n",
        "    api_key = input(\"Enter your OpenAI API key: \")\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5K1TnOmSRuH",
        "outputId": "bbd85ad7-ae11-4ad2-a308-5cf2c7bfff56"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Prepare Documents**"
      ],
      "metadata": {
        "id": "VtCp9q_6Sg-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV document (stored in the repo under /data folder)\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "# Path relative to your GitHub repo\n",
        "file_path = \"data/company_financials_quarterly.csv\"\n",
        "\n",
        "loader = CSVLoader(file_path=file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded\", len(docs), \"rows from document.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua9yyaskSiwi",
        "outputId": "b590bde9-cf59-488d-8dcf-ba8570a1c16f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 100 pages from document.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split data into Chunks, convert to Emebedding and store in vector DB**"
      ],
      "metadata": {
        "id": "5p12ZqGtS_Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(\"Split into\", len(chunks), \"chunks\")\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "knowledge_base = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = knowledge_base.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmdWv25TAi8",
        "outputId": "79f0d36e-d21a-48f9-effc-522765832ec1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 100 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup LLM and QA Chain**"
      ],
      "metadata": {
        "id": "gUpmN9CETPTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMfUrQMITRnG",
        "outputId": "a8348354-838c-4d2a-bf79-5cf3f6e26634",
        "collapsed": true
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1324555225.py:2: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define State for LangGraph**"
      ],
      "metadata": {
        "id": "m9_Fr0ZwTH9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    documents: List[str]\n",
        "    answer: str\n",
        "    intent: str"
      ],
      "metadata": {
        "id": "eWg5zlrpTNNa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Graph Nodes**"
      ],
      "metadata": {
        "id": "8c1VgxuXTTDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intent Detection Node (simple: all go to retrieval)\n",
        "def detect_intent(state: RAGState):\n",
        "    return {\"intent\": \"retrieval\"}\n",
        "\n",
        "# Retrieval Node\n",
        "def retrieve(state: RAGState):\n",
        "    docs = retriever.get_relevant_documents(state[\"question\"])\n",
        "    return {\"documents\": docs}\n",
        "\n",
        "# LLM Response Node\n",
        "def generate(state: RAGState):\n",
        "    answer = qa_chain.run(\n",
        "        {\"question\": state[\"question\"], \"input_documents\": state.get(\"documents\", [])}\n",
        "    )\n",
        "    return {\"answer\": answer}\n"
      ],
      "metadata": {
        "id": "wTqylCCXTVRb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build LangGraph Workflow with Memory**"
      ],
      "metadata": {
        "id": "NH6vCvRRTXwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(RAGState)\n",
        "\n",
        "graph.add_node(\"detect_intent\", detect_intent)\n",
        "graph.add_node(\"retrieve\", retrieve)\n",
        "graph.add_node(\"generate\", generate)\n",
        "\n",
        "graph.set_entry_point(\"detect_intent\")\n",
        "graph.add_conditional_edges(\n",
        "    \"detect_intent\",\n",
        "    lambda state: state[\"intent\"],\n",
        "    {\"retrieval\": \"retrieve\"},\n",
        ")\n",
        "graph.add_edge(\"retrieve\", \"generate\")\n",
        "graph.add_edge(\"generate\", END)\n",
        "\n",
        "# Memory\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "9iq-1GebTanx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the query engine (optional block)**"
      ],
      "metadata": {
        "id": "EgWXCixUTcwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread_id = \"demo_queries\"\n",
        "\n",
        "q1 = \"What information does the dataset contain?\"\n",
        "out1 = app.invoke({\"question\": q1}, config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "print(\"Q:\", q1)\n",
        "print(\"A:\", out1[\"answer\"])\n",
        "\n",
        "q2 = \"What was the revenue in Q2 2010?\"\n",
        "out2 = app.invoke({\"question\": q2}, config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "print(\"\\nQ:\", q2)\n",
        "print(\"A:\", out2[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlnupQUSTgEr",
        "outputId": "d8304249-8643-4bd4-cb1e-824d6a4ae776"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1030771296.py:7: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  docs = retriever.get_relevant_documents(state[\"question\"])\n",
            "/tmp/ipython-input-1030771296.py:12: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  answer = qa_chain.run(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q: What information does the dataset contain?\n",
            "A: The dataset contains financial information for a company over different years and quarters. This information includes revenue, expenses, profit, growth rate, and the number of employees for each quarter in the years 2000, 2001, 2004, and 2011.\n",
            "\n",
            "Q: What was the revenue in Q2 2010?\n",
            "A: The revenue in Q2 2010 was 194.23 million USD.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main thread - Interactive Chat**"
      ],
      "metadata": {
        "id": "6nRXjTF3Thj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread_id = \"company_revenue\"\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"User: \")\n",
        "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Chat ended.\")\n",
        "        break\n",
        "\n",
        "    output = app.invoke(\n",
        "        {\"question\": user_query},\n",
        "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
        "    )\n",
        "    print(\"AI:\", output[\"answer\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnhLY7YUTkQW",
        "outputId": "93655207-e499-4a50-e3ba-93d84ef4ed64"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User: WHat is the year which got least and the highest revenues?\n",
            "AI: The year with the least revenue is 2008 with a revenue of 227.88 Million USD, and the year with the highest revenue is 2013 with a revenue of 472.51 Million USD.\n",
            "User: exit\n",
            "Chat ended.\n"
          ]
        }
      ]
    }
  ]
}