{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Install Dependencies and Libraries**"
      ],
      "metadata": {
        "id": "3_iICbCKPJ2X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "jxTJKCmZPG8s"
      },
      "outputs": [],
      "source": [
        "!pip install -U -qq langchain langgraph langchain_openai langchain_community faiss-cpu\n",
        "!pip install -U -qq pypdf\n",
        "\n",
        "import os\n",
        "from typing import TypedDict, List\n",
        "\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "from langchain.chains.question_answering import load_qa_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup API Key**"
      ],
      "metadata": {
        "id": "A173Duo9SOWT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = input(\"Enter your OpenAI API key: \")\n",
        "os.environ[\"OPENAI_API_KEY\"] = api_key"
      ],
      "metadata": {
        "id": "_5K1TnOmSRuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and Prepare Documents**"
      ],
      "metadata": {
        "id": "VtCp9q_6Sg-7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV document (stored in the repo under /data folder)\n",
        "from langchain_community.document_loaders import CSVLoader\n",
        "\n",
        "# Path relative to your GitHub repo\n",
        "file_path = \"data/company_financials_quarterly.csv\"\n",
        "\n",
        "loader = CSVLoader(file_path=file_path, encoding=\"utf-8\")\n",
        "docs = loader.load()\n",
        "\n",
        "print(\"Loaded\", len(docs), \"rows from document.\")"
      ],
      "metadata": {
        "id": "ua9yyaskSiwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split data into Chunks, convert to Emebedding and store in vector DB**"
      ],
      "metadata": {
        "id": "5p12ZqGtS_Lw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "chunks = splitter.split_documents(docs)\n",
        "print(\"Split into\", len(chunks), \"chunks\")\n",
        "\n",
        "embeddings = OpenAIEmbeddings()\n",
        "knowledge_base = FAISS.from_documents(chunks, embeddings)\n",
        "retriever = knowledge_base.as_retriever()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMmdWv25TAi8",
        "outputId": "79f0d36e-d21a-48f9-effc-522765832ec1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split into 100 chunks\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup LLM and QA Chain**"
      ],
      "metadata": {
        "id": "gUpmN9CETPTz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMfUrQMITRnG",
        "outputId": "a8348354-838c-4d2a-bf79-5cf3f6e26634",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1324555225.py:2: LangChainDeprecationWarning: This class is deprecated. See the following migration guides for replacements based on `chain_type`:\n",
            "stuff: https://python.langchain.com/docs/versions/migrating_chains/stuff_docs_chain\n",
            "map_reduce: https://python.langchain.com/docs/versions/migrating_chains/map_reduce_chain\n",
            "refine: https://python.langchain.com/docs/versions/migrating_chains/refine_chain\n",
            "map_rerank: https://python.langchain.com/docs/versions/migrating_chains/map_rerank_docs_chain\n",
            "\n",
            "See also guides on retrieval and question-answering here: https://python.langchain.com/docs/how_to/#qa-with-rag\n",
            "  qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define State for LangGraph**"
      ],
      "metadata": {
        "id": "m9_Fr0ZwTH9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGState(TypedDict):\n",
        "    question: str\n",
        "    documents: List[str]\n",
        "    answer: str\n",
        "    intent: str"
      ],
      "metadata": {
        "id": "eWg5zlrpTNNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define Graph Nodes**"
      ],
      "metadata": {
        "id": "8c1VgxuXTTDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intent Detection Node (simple: all go to retrieval)\n",
        "def detect_intent(state: RAGState):\n",
        "    return {\"intent\": \"retrieval\"}\n",
        "\n",
        "# Retrieval Node\n",
        "def retrieve(state: RAGState):\n",
        "    docs = retriever.get_relevant_documents(state[\"question\"])\n",
        "    return {\"documents\": docs}\n",
        "\n",
        "# LLM Response Node\n",
        "def generate(state: RAGState):\n",
        "    answer = qa_chain.run(\n",
        "        {\"question\": state[\"question\"], \"input_documents\": state.get(\"documents\", [])}\n",
        "    )\n",
        "    return {\"answer\": answer}\n"
      ],
      "metadata": {
        "id": "wTqylCCXTVRb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build LangGraph Workflow with Memory**"
      ],
      "metadata": {
        "id": "NH6vCvRRTXwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = StateGraph(RAGState)\n",
        "\n",
        "graph.add_node(\"detect_intent\", detect_intent)\n",
        "graph.add_node(\"retrieve\", retrieve)\n",
        "graph.add_node(\"generate\", generate)\n",
        "\n",
        "graph.set_entry_point(\"detect_intent\")\n",
        "graph.add_conditional_edges(\n",
        "    \"detect_intent\",\n",
        "    lambda state: state[\"intent\"],\n",
        "    {\"retrieval\": \"retrieve\"},\n",
        ")\n",
        "graph.add_edge(\"retrieve\", \"generate\")\n",
        "graph.add_edge(\"generate\", END)\n",
        "\n",
        "# Memory\n",
        "memory = MemorySaver()\n",
        "app = graph.compile(checkpointer=memory)\n"
      ],
      "metadata": {
        "id": "9iq-1GebTanx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing the query engine (optional block)**"
      ],
      "metadata": {
        "id": "EgWXCixUTcwF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread_id = \"demo_queries\"\n",
        "\n",
        "q1 = \"What information does the dataset contain?\"\n",
        "out1 = app.invoke({\"question\": q1}, config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "print(\"\\n\\n**********\\n\")\n",
        "print(\"Q:\", q1)\n",
        "print(\"A:\", out1[\"answer\"])\n",
        "\n",
        "q2 = \"What was the revenue in Q2 2010?\"\n",
        "out2 = app.invoke({\"question\": q2}, config={\"configurable\": {\"thread_id\": thread_id}})\n",
        "print(\"\\nQ:\", q2)\n",
        "print(\"A:\", out2[\"answer\"])"
      ],
      "metadata": {
        "id": "PlnupQUSTgEr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main thread - Interactive Chat**"
      ],
      "metadata": {
        "id": "6nRXjTF3Thj0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "thread_id = \"company_revenue\"\n",
        "\n",
        "while True:\n",
        "    user_query = input(\"User: \")\n",
        "    if user_query.lower() in [\"exit\", \"quit\"]:\n",
        "        print(\"Chat ended.\")\n",
        "        break\n",
        "\n",
        "    output = app.invoke(\n",
        "        {\"question\": user_query},\n",
        "        config={\"configurable\": {\"thread_id\": thread_id}}\n",
        "    )\n",
        "    print(\"AI:\", output[\"answer\"])"
      ],
      "metadata": {
        "id": "MnhLY7YUTkQW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
